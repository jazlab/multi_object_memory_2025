"""Run this script to compute and cache spike counts.

Usage:
$ python3 run_spikes_per_trial.py

This computes a binned spike trail for each trial containing the spike counts in
10 ms bins going from stimulus onset to feedback onset for completed trials
for which the unit was observed. This array is saved to
"../cache/phys_processing/spikes_to_trials/spikes_per_trial/$SUBJECT/$SESSION/$PROBE/$QUALITY/$UNIT_spike_counts.npy"
It also saves a list of trial numbers corresponding to this array to
"../cache/phys_processing/spikes_to_trials/spikes_per_trial/$SUBJECT/$SESSION/$PROBE/$QUALITY/$UNIT_trials.npy"

This script requires the following cached data:
- Spike sorting data in "../cache/dandi_data/spikesorting". This can be
  downloaded from DANDI by running `../../download_dandi_data.py`.
- Behavior data for the triangle and ring tasks, saved in
  `../../cache/behavior/triangle.csv` and `../../cache/behavior/ring.csv`. This
  can be generated by running the script
  `../../behavior_processing/run_cache_data.py`.
"""

import pickle
from pathlib import Path

import constants
import numpy as np
import pandas as pd
from pynwb import NWBHDF5IO

_SPIKESORTING_DATA_DIR = Path("../../cache/dandi_data/spikesorting")
_WRITE_DIR = Path(
    "../../cache/phys_processing/spikes_to_trials/spikes_per_trial"
)
_BEHAVIOR_CACHE_PATH_RING = Path("../../cache/behavior/ring.csv")
_BEHAVIOR_CACHE_PATH_TRIANGLE = Path("../../cache/behavior/triangle.csv")


def _spikes_to_spike_counts(
    trial_spikes: list[float],
    trial_duration: float,
) -> np.ndarray:
    """Convert spike times to spike count vector.

    Args:
        trial_spikes: List of spike times in trial, relative to stimulus onset.
        trial_duration: Duration of trial between stimulus onset and feedback
            onset.

    Returns:
        spike_counts: Spike count vector of length trial_duration / bin_size.
            Each element is the number of spikes in the corresponding bin.
    """
    num_bins = 1 + int(trial_duration / constants.BIN_SIZE_SECONDS)
    spike_counts = np.zeros(num_bins)
    for spike_t in trial_spikes:
        bin_index = int(spike_t / constants.BIN_SIZE_SECONDS)
        spike_counts[bin_index] += 1
    return spike_counts


def _get_spike_counts(
    spike_times: list[float], trial_intervals: list[tuple[float, float]]
) -> list[list[float]]:
    """Get spike counts for each trial.

    Args:
        spike_times: List of spike times for a unit.
        trial_intervals: List of tuples of trial start and end times.

    Returns:
        spike_count_per_trial: List of spike count vectors for each trial.
    """
    spike_counts_per_trial = []
    spike_index = 0
    for trial_start, trial_end in trial_intervals:
        trial_spikes = []
        next_trial = False
        while not next_trial:
            if spike_index >= len(spike_times):
                # No more spikes for this neuron, so move on to next trial
                next_trial = True
                continue

            spike_t = spike_times[spike_index]
            if spike_t > trial_end:
                # Move on to next trial
                next_trial = True
                continue

            if spike_t < trial_start:
                # Have yet to reach beginning of trial, so skip this spike
                spike_index += 1
                continue

            # Append spike to trial and increment spike index
            trial_spikes.append(spike_t - trial_start)
            spike_index += 1

        # Convert trial_spikes to spike count and append
        spike_counts_per_trial.append(
            _spikes_to_spike_counts(trial_spikes, trial_end - trial_start)
        )

    return spike_counts_per_trial


def _session_spike_counts(
    spikesorting_file: Path,
    ring_behavior: pd.DataFrame,
    triangle_behavior: pd.DataFrame,
) -> None:
    """Compute and save spike counts for each unit in a session.

    Args:
        spikesorting_file: Path to NWB file with spike sorting data.
        ring_behavior: DataFrame with ring task behavior data.
        triangle_behavior: DataFrame with triangle task behavior data.
    """
    # Load neural data
    spikesorting_read_io = NWBHDF5IO(
        spikesorting_file, mode="r", load_namespaces=True
    )
    spikesorting_nwbfile = spikesorting_read_io.read()
    ecephys = spikesorting_nwbfile.processing["ecephys"]
    units = ecephys.data_interfaces["units"]
    print(f"Number of units = {len(units)}")

    # Get subject and session and make write directory
    subject = spikesorting_nwbfile.subject.subject_id
    session = spikesorting_nwbfile.session_id
    write_dir = _WRITE_DIR / subject / session
    write_dir.mkdir(parents=True, exist_ok=True)

    # Load behavior data for this session
    session_ring_behavior = ring_behavior[
        (ring_behavior["subject"] == subject)
        & (ring_behavior["session"] == session)
    ]
    session_triangle_behavior = triangle_behavior[
        (triangle_behavior["subject"] == subject)
        & (triangle_behavior["session"] == session)
    ]
    if len(session_ring_behavior) > 0:
        print(f"Ring behavior data found for {subject} {session}")
        session_behavior = session_ring_behavior
    elif len(session_triangle_behavior) > 0:
        print(f"Triangle behavior data found for {subject} {session}")
        session_behavior = session_triangle_behavior
    else:
        print(f"No behavior data found for {subject} {session}")
        return
    print(f"Number of trials = {len(session_behavior)}")

    # Iterate through units and generate unit summary plots
    for unit_index in range(len(units)):
        if unit_index % 20 == 0:
            print(f"Processing unit {unit_index} / {len(units)}")

        # Load spike times and amplitudes
        spike_times = units.spike_times_index[unit_index]
        electrodes_group = units.electrodes_group[unit_index]
        quality = units.quality[unit_index]
        obs_trials = units.obs_trials_index[unit_index]

        # Make write paths
        unit_write_dir = write_dir / electrodes_group / quality
        fr_write_path = unit_write_dir / f"{unit_index}_spike_counts.pkl"
        trials_write_path = unit_write_dir / f"{unit_index}_trials.pkl"
        if fr_write_path.exists():
            continue

        # Get trial numbers and intervals for completed observed trials
        trial_numbers = []
        trial_intervals = []
        for _, trial in session_behavior.iterrows():
            if not obs_trials[trial.trial_num]:
                continue
            if not trial.completed:
                continue
            trial_numbers.append(trial.trial_num)
            trial_intervals.append(
                (
                    trial.time_stimulus_onset - constants.LEAD_IN_SECONDS,
                    trial.time_feedback_onset,
                )
            )

        # Get spike counts for each trial
        spike_count_per_trial = _get_spike_counts(spike_times, trial_intervals)
        spike_count_per_trial = [x.tolist() for x in spike_count_per_trial]

        # Save spike counts and trial numbers
        if not fr_write_path.parent.exists():
            fr_write_path.parent.mkdir(parents=True, exist_ok=True)
        pickle.dump(trial_numbers, open(trials_write_path, "wb"))
        pickle.dump(spike_count_per_trial, open(fr_write_path, "wb"))


def main():
    """Generate and save unit summary plots."""
    # Load behavior data
    ring_behavior = pd.read_csv(_BEHAVIOR_CACHE_PATH_RING)
    triangle_behavior = pd.read_csv(_BEHAVIOR_CACHE_PATH_TRIANGLE)

    # Append each session to the dataframe
    for spikesorting_subject_dir in sorted(_SPIKESORTING_DATA_DIR.iterdir()):
        if spikesorting_subject_dir.name.startswith("."):
            continue
        print(f"\nProcessing {spikesorting_subject_dir.name}\n")
        for spikesorting_file in sorted(spikesorting_subject_dir.iterdir()):
            print(f"\nProcessing {spikesorting_file.name}\n")
            if spikesorting_file.name.startswith("."):
                continue
            _session_spike_counts(
                spikesorting_file, ring_behavior, triangle_behavior
            )


if __name__ == "__main__":
    main()
